## D√©veloppement d‚Äôagents autonomes et cr√©ation de nouvelles r√®gles pour jeux de plateau via l‚Äôapprentissage par renforcement

Projet realis√© dans le cadre de l'UE "Projet Master" en Master 2 Sciences des Donn√©es et Syst√®mes Complexes par:
- KRUZIC Charlotte
- MARQUIS Zo√©
- KUDRIASHOV Daniil
- ZAITCEVA Ekaterina

## Description du Projet üéÆü§ñ

Ce projet explore l'apprentissage par renforcement appliqu√© √† des jeux de plateau, avec un focus sur le c√©l√®bre jeu de soci√©t√© Ludo (√©galement connu sous le nom de "Petits Chevaux"). Initialement, nous avions exp√©riment√© avec le jeu Labyrinthe, mais ce choix a √©t√© abandonn√© en raison de contraintes sp√©cifiques, comme expliqu√© dans la documentation.

---

### Description reformul√©e du projet

Le projet consiste √† d√©velopper des agents autonomes capables de jouer √† un jeu de plateau simul√© informatiquement en utilisant des techniques d‚Äôapprentissage par renforcement. L‚Äôobjectif est double : d‚Äôune part,
entra√Æner des agents √† optimiser leurs strat√©gies de jeu en fonction des r√®gles, et d‚Äôautre part, utiliser ces
agents comme outils d‚Äôanalyse pour explorer les diff√©rentes r√®gles de jeu.

TODO d'ici √†(
#### Caract√©ristiques principales
- Conception de joueurs autonomes : Les agents sont entra√Æn√©s √† naviguer dans un environnement
complexe et √† prendre des d√©cisions strat√©giques en temps r√©el, en s‚Äôappuyant sur un environnement
simul√©.
- Analyse des r√®gles existantes et proposition de nouvelles r√®gles : En modifiant les r√®gles du jeu, explorer leur impact sur l‚Äô√©quilibre, la jouabilit√©, et l‚Äôexp√©rience des joueurs.
- Personnalisation des comportements : Chaque agent peut √™tre param√©tr√© avec un style de jeu sp√©cifique (agressif, d√©fensif, al√©atoire, strat√©gique, etc.), permettant une diversit√© de simulations et d‚Äôinteractions.
- Application multi-usage :
    - Formation des agents via l‚Äôapprentissage par renforcement.
    - Simulation massive de parties pour tester de nouvelles m√©caniques ou √©valuer la difficult√© et l‚Äô√©quilibre des r√®gles.
    - Validation empirique de strat√©gies pour identifier celles qui conviennent √† diff√©rents profils de joueurs.

#### Objectifs identifi√©s
Le projet repose sur plusieurs objectifs techniques et analytiques, visant √† d√©velopper des agents autonomes
tout en approfondissant la compr√©hension des dynamiques des jeux de plateau :
1. Entra√Æner des agents RL pour jouer efficacement
    - Impl√©menter un environnement, permettant aux agents d‚Äôinteragir avec le jeu.
    - Utiliser des algorithmes d‚Äôapprentissage par renforcement pour optimiser les d√©cisions des  agents dans diff√©rents sc√©narios.
2. Effectuer des simulations massives pour tester diff√©rentes m√©caniques de jeu
    - Automatiser des milliers de parties avec des agents divers pour analyser les r√©sultats statistiquement.
    - Identifier les r√®gles ou configurations qui d√©s√©quilibrent le jeu ou le rendent impraticable.
    - Simuler des parties avec des agents h√©t√©rog√®nes pour √©valuer les interactions et l‚Äô√©quilibre g√©n√©ral.
3. Personnaliser les agents selon divers styles de jeu et comportements strat√©giques
    - Cr√©er des agents avec des pr√©f√©rences ou des objectifs sp√©cifiques.
    - Comparer l‚Äôefficacit√© des styles et identifier ceux favoris√©s dans diff√©rentes configurations.
4. Tester diff√©rentes strat√©gies et configurations de r√®gles
    - √âtudier les impacts des changements de r√®gles sur le gameplay.
    - D√©finir des m√©triques de ‚Äùjouabilit√©‚Äù telles que l‚Äô√©quilibre des forces, la dur√©e des parties, ou la diversit√© des strat√©gies possibles.
5. D√©velopper une interface graphique pour pouvoir jouer contre ces agents

TODO d'ici)

## Fonctionnalit√©s principales :
üß† Cr√©ation d'agents : Plusieurs agents ont √©t√© d√©velopp√©s, utilisant notamment l'algorithme Proximal Policy Optimization (PPO) pour optimiser leurs strat√©gies.  
‚öôÔ∏è Entra√Ænement des agents : Les agents ont √©t√© entra√Æn√©s sur des environnements simul√©s, avec des r√®gles vari√©es pour mod√©liser diff√©rents sc√©narios de jeu.  
üé≤ Simulation de parties : Nous avons simul√© des affrontements entre agents pour analyser leurs performances dans diff√©rents contextes, tout en testant les impacts des variations de r√®gles.  
üìä Analyse des performances : Une analyse approfondie des r√©sultats a √©t√© r√©alis√©e √† l'aide de techniques statistiques et des outils d√©di√©s. 
üéÆ Interface graphique interactive : L‚Äôinterface graphique d√©velopp√©e permet √† l‚Äôutilisateur humain d‚Äôaffronter les agents directement ou de simuler des parties entre humains.

---

## R√®gles du Jeu et Variations üìùüé≤

### R√®gles de Base :

- Chaque joueur commence avec tous ses pions dans une √©curie.
- Un 6 au d√© est requis pour sortir un pion de l'√©curie.
- Une fois sur le plateau, les pions doivent avancer sur un chemin commun de 56 cases, o√π :
    - Les pions peuvent se croiser ou se faire tuer en arrivant exactement sur une case occup√©e par un pion adverse.
    - R√®gles pour les d√©placements :
        - **Tuer un pion adverse** : Un pion peut √©liminer un pion adverse uniquement s‚Äôil tombe exactement sur la m√™me case.
        - **Rejoindre un pion alli√©** : Un pion appartenant au m√™me joueur peut rejoindre un autre pion uniquement si le lanc√© de d√© correspond exactement √† la distance entre les deux.
        - **Rester bloqu√© derri√®re un pion** : Si la valeur du d√© est sup√©rieure au nombre de cases jusqu‚Äôau pion suivant sur le plateau (qu‚Äôil appartienne au m√™me joueur ou √† un adversaire), le pion avancera jusqu'√† la case pr√©c√©dent l'obstacle. Les d√©passements de pions (alli√©s ou adverses) sont donc interdits. Ces r√®gles ne s'appliquent pas √† l'escalier.
- Chaque joueur poss√®de un escalier unique de 6 cases menant √† une case objectif.

- **Disposition des √©curies selon le nombre de joueurs** :
    - **2 joueurs** : Les √©curies sont plac√©es √† l'oppos√© l'une de l'autre sur le plateau. Ainsi, la case 1 du chemin pour un joueur correspond √† la case 29 pour l'autre.
    - **3 ou 4 joueurs** : Les √©curies sont r√©parties de mani√®re √©quidistante toutes les 14 cases. Une m√™me case peut √™tre per√ßue diff√©remment selon le point de vue du joueur :
        Par exemple, la case 1 pour un joueur sera la case 15, case 29 ou case 43 pour les autres joueurs, en fonction de leur position de d√©part.
        
    Cela garantit une r√©partition √©quilibr√©e des positions de d√©part sur le plateau.

### Variations des R√®gles :
- Nombre de joueurs :
    - Le jeu peut √™tre jou√© √† 2, 3 ou 4 joueurs.
- Nombre de pions par joueur :
    - Chaque joueur peut avoir entre 2 et 6 petits chevaux en jeu.
- Conditions de victoire :
    - Victoire rapide : Le premier joueur √† atteindre l‚Äôobjectif avec un seul pion gagne.
    - Victoire compl√®te : Tous les pions d‚Äôun joueur doivent atteindre l‚Äôobjectif pour d√©clarer sa victoire.
- R√®gles pour atteindre le pied de l'escalier :
    - Exactitude n√©cessaire : Un pion doit atteindre exactement la case situ√©e au pied de l'escalier pour commencer √† le gravir.
        - Si le lanc√© de d√© d√©passe la distance requise pour atteindre cette case, le pion peut avancer puis reculer, √† condition que ce mouvement r√©duise la distance qui le s√©pare de la case au pied de l'escalier.
        - Si, en avan√ßant puis reculant selon la valeur du d√©, le pion finit par s'√©loigner davantage de la case pied de l'escalier, alors il ne peut pas √™tre d√©plac√©.
    - Progression simplifi√©e : Si la valeur du d√© d√©passe le pied de l‚Äôescalier, le pion grimpe directement comme si l‚Äôescalier faisait partie du chemin.
- Ordre de progression sur l'escalier :
    - Ordre simplifi√© : Un pion peut monter plusieurs marches de l'escalier en un seul lanc√© de d√©, il suffit qu'il arrive ou d√©passe l'objectif pour l'atteindre.
    - Dans le cas de l'exactitude n√©cessaire pour le pied de l'excalier, on peut utiliser l'ordre simplifi√© ou alors l'ordre strict : 
        - Chaque marche de l'escalier n√©cessite un jet sp√©cifique : 1 pour la premi√®re marche, 2 pour la deuxi√®me, ... ainsi que 6 pour atteindre l‚Äôobjectif.
- Dans le cas de l'ordre strict pour progresser dans l'escalier : 
    - Rejouer lors de la mont√©e de chaque marche (oui ou non)

- Rejouer si d√© = 6 (oui ou non)

- Pouvoir prot√©ger un pion (oui ou non) : si on a deux pions sur la m√™me case, alors personne ne peut les tuer.

## Diff√©rents agents : 

Dans notre projet, nous avons d√©velopp√© six types d'agents distincts, chacun avec sa propre strat√©gie et son syst√®me de r√©compenses :

- **Agent Balanced (√âquilibr√©)** : Agent utilisant une strat√©gie √©quilibr√©e entre attaque et progression, servant de r√©f√©rence pour comparer les autres agents.

- **Agent Aggressive (Agressif)** : Agent privil√©giant les actions offensives avec des r√©compenses maximales pour l'√©limination des pions adverses.

- **Agent Rusher (Press√©)** : Agent focalis√© sur la progression rapide vers l'objectif avec un syst√®me de r√©compenses favorisant les mouvements vers l'avant et l'atteinte du but.

- **Agent Defensive (D√©fensif)** : Agent priorisant la s√©curit√© des pions avec des r√©compenses importantes pour l'atteinte des zones s√ªres et la protection mutuelle des pions.

- **Agent Spawner (Sortie rapide)** : Agent concentr√© sur la sortie rapide des pions de l'√©curie avec des r√©compenses maximales pour les actions de sortie.

- **Agent Suboptimal (Sous-optimal)** : Agent simulant un joueur inexp√©riment√© avec des r√©compenses plus √©lev√©es pour les actions g√©n√©ralement moins efficaces.

Chaque agent dispose de sa propre table de r√©compenses, adapt√©e selon sa strat√©gie. Cette diversit√© permet d'explorer diff√©rentes approches du jeu et d'√©tudier leur efficacit√© relative dans diff√©rentes configurations de r√®gles.

### Mindmap r√©sumant les diff√©rentes configurations

![Mindmap des r√®gles du jeu](/docs/mindmap_setup.png)

---

## Technologies utilis√©es :
üêç Python : Langage principal pour la gestion du jeu et des agents.  
üõ†Ô∏è Gymnasium : Environnements personnalis√©s pour l'apprentissage par renforcement.  
ü§ñ Stable-Baselines3 : Biblioth√®que utilis√©e pour entra√Æner les agents sur les environnements Gymnasium.  
üóÑÔ∏è PostgreSQL : Base de donn√©es pour stocker les r√©sultats des simulations et les m√©triques des agents.  
üìä Pandas et Jupyter Notebook : Analyse et visualisation des performances des agents.  
üé® Pygame : Interface graphique pour visualiser les parties en temps r√©el.  
‚úÖ Pytest : Tests unitaires pour garantir la fiabilit√© du code.  

--- 

## Installation 

#### Version de Python 
Nous avons utilis√© **Python 3.11** pour ce projet.
Assurez-vous que cette version est install√©e sur votre machine : 

##### Installation de Python 3.11
- MacOS (via Homebrew) : 
```bash
brew install python@3.11
```

- Linux (Ubuntu):
```bash
sudo apt update
sudo apt install python3.11
```

##### V√©rification de l'installation : 
```bash
python3.11 --version
```

#### Cr√©ation de l'environnement virtuel
Pour garantir la compatibilit√© et faciliter les tests, nous avons utilis√© `venv`.

##### V√©rifier la disponibilit√© de `venv``
```bash
python3.11 -m venv --help
```
- Si cette commande fonctionne, vous pouvez continuer.
- Sinon installez `venv`:
    - Sous MacOS, lorsque Python 3.11 est install√© via Homebrew, le module `venv`est inclus par d√©faut. Si la commande pr√©c√©dent a g√©n√©r√© une erreur : 
    ```bash
    brew update
    brew upgrade python
    brew reinstall python@3.11
    ```

    - Linux (Ubuntu)
    ```bash
    sudo apt update
    sudo apt install python3-venv
    ```


##### √âtapes pour cr√©er et configurer l'environnement virtuel 
- Cr√©er l'environnement virtuel
Depuis la racine du projet : 
```bash
python3.11 -m venv ludo_venv
```

- Activer l'environnement virtuel 
    ```bash
    source ludo_venv/bin/activate
    ```

- Mettre √† jour `pip` dans l'environnement virtuel
```bash
pip install --upgrade pip
```

- Installer les d√©pendances 
```bash
pip install -r requirements_venv.txt
```

##### D√©sactiver l'environnement virtuel
Une fois que vous avez termin√© vos tests ou que vous n'avez plus besoin d'utiliser l'environnement virtuel, vous pouvez le d√©sactiver facilement. Cela vous permettra de revenir √† votre environnement Python global ou de syst√®me.

Pour d√©sactiver l'environnement virtuel, ex√©cutez simplement la commande suivante :
```bash
deactivate
```

Cela d√©sactive l'environnement virtuel actif sans supprimer ses fichiers. Vous pourrez le r√©activer ult√©rieurement si n√©cessaire.

### Utilisation de l'environnement virtuel dans les notebooks 

Pour les analyses, exp√©rimentations et entra√Ænements, nous avons utilis√© des notebooks Jupyter via VSCode. Si vous souhaitez ex√©cuter un notebook dans le cadre de ce projet, nous vous recommandons d‚Äôutiliser VSCode, car nous n'avons pas test√© cette configuration avec d'autres outils ou √©diteurs.

Le package `ipykernel`, n√©cessaire pour connecter l'environnement virtuel aux notebooks Jupyter, est d√©j√† inclus dans les d√©pendances list√©es dans le fichier `requirements_venv.txt`.

##### √âtapes pour configurer le kernel dans VSCode : 

- Ouvrez un notebook `.ipynb` dans VSCode
- Cliquez sur **Run All** ou sur l'option **Select Kernel** situ√©e en haut √† droite de l'interface.
- Dans le menu qui s'affiche, cliquez sur **Select Another Kernel...**
- Dans la section **Python Envrionments**, choisissez l'environnement virtuel correspondant (`ludo_venv`)

Une fois ces √©tapes termin√©es, le notebook sera configur√© pour utiliser l'environnement virtuel, et vous pourrez ex√©cuter vos analyses en toute compatibilit√© avec les d√©pendances du projet.

---

## Lancer une partie avec interface graphique
Pour jouer avec l'interface graphique, placer vous dans le dossier `game``

    ```bash
    cd game
    ```

Puis ex√©cuter le fichier `play.py` comme ceci:

    ```bash
    python3 play_pygame/play.py
    ```

Voici un extrait d'une partie montrant un joueur humain et trois agents en action.
![Demo of the app](docs/demo_assets/demo_pc_1humain_3agents.gif)

#### Agents Non Entra√Æn√©s
Apr√®s avoir configur√© les r√®gles du jeu selon vos pr√©f√©rences, il est possible que l'agent correspondant √† cette configuration ne soit pas encore entra√Æn√©. Si un agent non entra√Æn√© est utilis√©, le programme √©chouera.
Dans ce cas, il faut relancer le programme et choisir une configuration de r√®gles diff√©rente.

**Message d'Erreur Attendu**

Voici un exemple de message d'erreur qui peut appara√Ætre dans ce cas :
```
"Le fichier <model_file_name> n'existe pas. Veuillez entra√Æner l'agent avant de l'utiliser."
```
---

## Tests avec Pytest

Afin de garantir que la logique du jeu est robuste et fonctionne comme pr√©vu, nous avons mis en place des tests unitaires avec Pytest. Ces tests couvrent diff√©rents aspects de la logique du jeu pour s'assurer que chaque fonctionnalit√© est correctement impl√©ment√©e.

#### Lancer les tests Pytest 
Pour ex√©cuter les tests, utilisez la commande suivante √† la racine du projet :
```bash
pytest game/tests_pytest/
```

#### R√©sultat attendu
Si tous les tests passent avec succ√®s, vous devriez voir une sortie similaire √† celle-ci :

```bash
============================== 82 passed, 1 warning in 1.30s ==============================
```

Cela indique que 82 tests ont √©t√© valid√©s avec succ√®s. Le warning peut √™tre d√ª √† une d√©pendance ou une configuration et ne devrait pas affecter le fonctionnement principal du jeu.

---


## R√©sultats et analyses

### Base de donn√©es et simulations de parties

#### Base de donn√©es

Le projet s‚Äôappuie sur une base de donn√©es relationnelle PostgreSQL pour collecter, structurer et analyser les donn√©es g√©n√©r√©es lors des simulations de parties. Cette base de donn√©es centralise des informations pertinentes sur les joueurs, les parties, les r√®gles et les actions effectu√©es.

![Sch√©ma de la base de donn√©es](docs/schema_db.jpg)

#### Simulations de parties

Les simulations de parties sont ex√©cut√©es de mani√®re automatis√©e gr√¢ce au fichier `ludo_stats_play.py`. Ce script est con√ßu pour lancer un grand nombre de parties entre agents, avec pour objectif principal d‚Äôenregistrer les donn√©es g√©n√©r√©es lors de chaque partie. Les statistiques collect√©es, telles que les performances des agents, leurs actions, leurs scores, et les r√©sultats des parties, sont automatiquement sauvegard√©es dans la base de donn√©es. Ces informations servent ensuite √† l'analyse des performances et des comportements des agents.

Les donn√©es g√©n√©r√©es lors des simulations sont enregistr√©es dans la base de donn√©es, puis export√©es au format CSV pour faciliter leur analyse dans des notebooks Python. Ces fichiers constituent la base d‚Äôanalyses d√©taill√©es, permettant de visualiser des m√©triques cl√©s telles que les scores obtenus, les taux d‚Äôerreurs, ou encore la r√©partition des types d‚Äôactions effectu√©es par les agents. Ces analyses visent √† d√©montrer l‚Äôefficacit√© de l‚Äôentra√Ænement des agents, leurs performances, mais aussi √† identifier les marges d‚Äôam√©lioration dans leurs comportements et strat√©gies.

### Analyse de l'entrainement des agents 

NB : L'analyse compl√®te de l'entrainement est disponible dans le notebook `db/analyse/analyse_entrainement.ipynb`.

#### Objectif

√âtudier l'√©volution des performances d'agents RL en fonction du nombre de pas d'entra√Ænement, √† travers des configurations de jeu de complexit√© croissante.

#### Donn√©es et param√®tres de l'analyse
**Donn√©es**  
Nous avons analys√© les donn√©es collect√©es suite aux parties massives, entre agents identiques (m√™me nombre de pas d'entrainement et m√™me type), lanc√©es avec la fonction `main_lancer_parties_pour_analyse_entrainement()` du fichier `ludo_stats_play.py`.

**M√©triques**  
- R√©partition des types d'actions demand√©es
- Pourcentage moyen d'erreurs
- Score moyen des agents

**Configurations**  
- Configuration 16 : R√®gles de base sans contraintes
- Configuration 12 : R√®gles interm√©diaires avec contraintes
- Configuration 17 : R√®gles compl√®tes avec interactions avanc√©es

**Nombre et types d'agents**  
Tous les agents ont √©t√© √©tudi√©s (Balanced, Aggressive, Rusher, Defensive, Spawner, et Suboptimal) dans des parties avec : 2 joueurs et 2 pions, 2 joueurs et 4 pions, et 4 joueurs et 4 pions. Nous avons entrain√©, fait jouer, et analys√© les performances de ces agents avec 50 000, 100 000, 200 000, et 400 000 pas d'entrainement.

#### R√©sultats

Les r√©sultats ont montr√©s que d'une mani√®re g√©n√©rale, l'entra√Ænement des agents permet l'apprentissage progressif de nouvelles actions, confirmant que le processus d'entra√Ænement fonctionne. Les agents apprennent √† s'adapter √† leur environnement et √† ma√Ætriser les r√®gles associ√©es.

Pour la **configuration 16**, la plus simple, les agents montrent des performances claires et stables. Le pourcentage moyen d'erreurs diminue, et les scores augmentent au cours de l'entrainement, indiquant une bonne capacit√© d'adaptation √† l'environnement. En revanche, l'augmentation du nombre de joueurs et de pions provoque un ralentissement de l'entrainement. 

Exemple pour l'agent aggressive :
![16-2-2-aggressive](/docs/graphiques_analyse_entrainement/16-2-2-aggressive.png)
![16-2-4-aggressive](/docs/graphiques_analyse_entrainement/16-2-4-aggressive.png)
![16-4-4-aggressive](/docs/graphiques_analyse_entrainement/16-4-4-aggressive.png)

Avec l'introduction de r√®gles suppl√©mentaires, les performances des agents deviennent moins stables. Bien que certains agents continuent de progresser, d'autres montrent des fluctuations dans leurs taux d'erreurs et leurs scores. 

Pour la **configuration 12**, l'entra√Ænement reste efficace, mais il est moins concluant que pour la configuration 16. Les agents apprennent certaines actions adapt√©es aux nouvelles r√®gles, mais leur capacit√© d'adaptation est moins marqu√©e, et les performances globales restent limit√©es.

Exemple de l'agent aggressive :
![12-2-2-aggressive](/docs/graphiques_analyse_entrainement/12-2-2-aggressive.png)

- Actions 50000 pas : NO_ACTION
- Nouvelles actions 100000 : MOVE_FORWARD, MARCHE_2
- Nouvelles actions 200000 : KILL
- Nouvelles actions 400000 : AVANCE_RECULE_PIED_ESCALIER

Exemple de diff√©rence entre les agents aggressive et balanced :
![12-4-4-aggressive](/docs/graphiques_analyse_entrainement/12-4-4-aggressive.png)
![12-4-4-balanced](/docs/graphiques_analyse_entrainement/12-4-4-balanced.png)

Enfin, dans la **configuration 17**, la plus complexe, les agents rencontrent les plus grandes difficult√©s. Le pourcentage moyen d'erreurs reste √©lev√©, et les scores moyens restent tr√®s faibles, montrant une incapacit√© des agents √† apprendre les r√®gles du jeu. De plus, l'apprentissage de nouvelles actions est plus lent.

Exemple de l'agent aggressive :
![17-2-2-aggressive](/docs/graphiques_analyse_entrainement/17-2-2-aggressive.png)

- Actions 50000 pas : NO_ACTION
- Nouvelles actions 100000 : MOVE_FORWARD
- Nouvelles actions 200000 : MARCHE_6
- Nouvelles actions 400000 : MOVE_OUT


Ces r√©sultats montrent que les agents s'entrainent bien pour des configurations de r√®gles simples, mais mettent cependant en √©vidence les limites des agents RL √† s'adapter √† des environnements de plus en plus complexes.

### Analyse des performances des agents

L'analyse compl√®te des performances des agents est disponible dans le notebook `db/analyse/analyse_agents.ipynb`. Nous pr√©sentons ici une synth√®se des r√©sultats principaux.

L'√©tude des performances des agents dans notre jeu repr√©sente un d√©fi analytique important en raison du nombre consid√©rable de configurations possibles : 32 configurations de r√®gles diff√©rentes, parties √† 2 ou 4 joueurs, 6 types d'agents, et possibilit√©s de 2 ou 4 pions par joueur. Nous nous sommes donc concentr√©s sur les configurations les plus repr√©sentatives pour illustrer nos analyses.

#### Performance des agents au sein d'une configuration fixe

Pour la configuration 8, qui offre un √©quilibre int√©ressant entre les diff√©rentes strat√©gies, nous avons analys√© les performances relatives des agents.

![Matrice des taux de victoire entre agents](/docs/graphiques_analyse_agents/taux_victoire_conf8.png)

*Figure 1: Matrice des taux de victoire entre agents (configuration 8)*

Cette matrice r√©v√®le des diff√©rences significatives dans l'efficacit√© des strat√©gies. Par exemple, l'agent aggressive montre une forte domination contre l'agent defensive (62% de victoires) mais peine davantage face √† l'agent rusher (49% de victoires).

Dans les parties √† quatre joueurs, la dynamique change significativement :

![Distribution des positions finales](/docs/graphiques_analyse_agents/distribution_places_conf8.png)

*Figure 2: Distribution des positions finales par type d'agent en parties √† 4 joueurs*

Face √† des adversaires multiples, les √©carts de performance se r√©duisent. L'agent defensive atteint la premi√®re place aussi souvent que l'agent aggressive, mais montre des performances moindres pour les deuxi√®mes places.

#### Satisfaction des agents selon les r√®gles

La satisfaction des agents a √©t√© √©valu√©e selon des m√©triques sp√©cifiques √† chaque strat√©gie. Pour l'agent aggressive par exemple :

![Satisfaction de l'agent aggressive](/docs/graphiques_analyse_agents/satisfaction_aggressive.png)

*Figure 3: Nombre moyen d'√©liminations par partie selon les configurations*

Les configurations 11 et 12, avec leurs contraintes plus strictes sur la progression vers l'objectif, permettent plus d'√©liminations et donc une meilleure satisfaction pour l'agent aggressive. Cependant, ce taux de satisfaction √©lev√© ne correspond pas toujours √† un meilleur taux de victoire.

#### √âquilibre et jouabilit√©

L'analyse de la configuration 7 r√©v√®le des aspects int√©ressants sur l'√©quilibre du jeu :

![Longueur des parties](/docs/graphiques_analyse_agents/longueur_parties_conf7.png)

*Figure 4: Distribution des longueurs de parties selon les types d'agents*

La dur√©e des parties reste remarquablement stable quelle que soit la combinaison d'agents, sugg√©rant un bon √©quilibre g√©n√©ral.

Cependant, l'analyse des parties 2v2 montre une autre r√©alit√© :

![Distribution des pions en 2v2](/docs/graphiques_analyse_agents/pions_objectif_2v2.png)

*Figure 5: Distribution du nombre de pions √† l'objectif par √©quipe*

La dispersion des points r√©v√®le que certaines combinaisons d'agents sont nettement plus efficaces que d'autres pour atteindre l'objectif, indiquant des d√©s√©quilibres dans cette configuration.


---

## Arborescence du projet

### √Ä la racine 

Voici la structure des principaux dossiers et fichiers de ce projet, avec une description de leur contenu et r√¥le.

```bash
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ db/
‚îú‚îÄ‚îÄ game/
‚îú‚îÄ‚îÄ ludo_venv/
‚îî‚îÄ‚îÄ requirements_venv.txt
```

#### Description des dossiers et fichiers

- `README.md` : Contient la documentation principale du projet, y compris les objectifs, instructions d'installation et exemples d'utilisation.
- `docs/` : Utilis√© tout au long du semestre pour centraliser les recherches, notes, explications, choix d'√©quipe et toute trace √©crite utile √† communiquer.
Contient √©galement les fichiers de documentation compl√©mentaires, tels que :
    - Des explications techniques sur le projet.
    - Des captures d'√©cran ou diagrammes pour illustrer les concepts cl√©s.

- `db/` : Ce dossier stocke les fichiers n√©cessaires √† la gestion et √† l‚Äôexploitation des donn√©es du projet. Il contient :
    - Les scripts pour initialiser la base de donn√©es, ins√©rer des donn√©es, et les exporter au format CSV.
    - Les donn√©es export√©es utilis√©es pour les analyses.
    - Des notebooks pour analyser les performances et l‚Äôentra√Ænement des agents.
    - Les fichiers de configuration de la base PostgreSQL.
    - Des scripts pour g√©rer les r√®gles du jeu et les configurations des parties simul√©es.
- `game/`: Ce dossier constitue le c≈ìur du projet et contient :
    - La logique interne du jeu, ainsi que l'environnement Gym attach√© pour les simulations et l'apprentissage par renforcement.
    - Les fichiers n√©cessaires pour lancer le jeu avec une interface graphique.
    - Des notebooks d√©di√©s √† l'apprentissage automatique, permettant d'entra√Æner et d'√©valuer des agents.
    - Des tests en pytest pour garantir que la logique du jeu respecte les r√®gles d√©finies.
- `requirements_venv.txt` : Une version sp√©cifique des d√©pendances utilis√©e avec l‚Äôenvironnement virtuel.

### `game/`

```bash
game/
‚îú‚îÄ‚îÄ __init__.py                    
‚îú‚îÄ‚îÄ environment.yml                
‚îú‚îÄ‚îÄ images/                        
‚îú‚îÄ‚îÄ ludo_env/                    
‚îú‚îÄ‚îÄ play_pygame/    
‚îú‚îÄ‚îÄ reinforcement_learning/        
‚îî‚îÄ‚îÄ tests_pytest/                  
```

- `__init__.py` : Fichier d'initialisation pour le module Python.
- `environment.yml`: Fichier de configuration pour recr√©er l'environnement conda.
- `images/` : Contient les images utilis√©es pour l'interface graphique.
-  `ludo_env/`: Ce r√©pertoire contient l‚Äôimpl√©mentation compl√®te de l‚Äôenvironnement Gym pour le jeu Ludo, incluant :
    - La logique du jeu.
    - La gestion des √©tats et actions.
    - L'int√©gration avec Gym pour permettre l‚Äôentra√Ænement d‚Äôagents RL.
- `play_pygame/`: Dossier contenant le code pour jouer au jeu avec une interface graphique d√©velopp√©e avec Pygame.
- `reinforcement_learning/` : Inclut les notebooks et scripts relatifs √† l'apprentissage par renforcement.
- `tests_pytest/`: Contient les tests unitaires √©crits avec pytest pour s'assurer que :
    - Les r√®gles du jeu sont correctement impl√©ment√©es.
    - Les actions de l‚Äôenvironnement respectent les contraintes d√©finies.
    - Les r√©sultats sont conformes aux attentes pour diff√©rents sc√©narios.

### `game/ludo_env`

```bash
game/                     
‚îî‚îÄ‚îÄ ludo_env/                    
    ‚îú‚îÄ‚îÄ __init__.py              
    ‚îú‚îÄ‚îÄ __pycache__/             
    ‚îú‚îÄ‚îÄ action.py                
    ‚îú‚îÄ‚îÄ env.py                   
    ‚îú‚îÄ‚îÄ game_logic.py            
    ‚îú‚îÄ‚îÄ renderer.py             
    ‚îú‚îÄ‚îÄ reward.py                
    ‚îî‚îÄ‚îÄ state.py                 
```

- `__init__.py` : Ce fichier fait de ludo_env un module Python. Il permet d'importer facilement les fichiers du r√©pertoire dans d'autres parties du projet.
- `action.py` : D√©finit les actions disponibles pour les agents dans le jeu.
- `env.py` : L‚Äôenvironnement Gym au c≈ìur du projet
    -  Le fichier env.py est une composante centrale de notre impl√©mentation. Il constitue une interface standardis√©e pour :
        - Jouer au jeu Ludo entre humains via une interface graphique ou textuelle.
        - Effectuer des entra√Ænements en apprentissage par renforcement (RL).
        - Simuler des milliers de parties afin de collecter des donn√©es statistiques ou √©valuer les performances des agents.

    - Fonctions principales de env.py 
        - `reset()`: Initialise une nouvelle partie et met l‚Äôenvironnement dans son √©tat de d√©part.
    Retourne l‚Äô√©tat initial du plateau sous une forme exploitable par l‚Äôagent RL ou par des simulations.
        - `step(action)`: Re√ßoit une action (propos√©e par un agent ou un humain).
    Ex√©cute cette action, calcule les cons√©quences (r√©compense, √©tat suivant, fin de partie, etc.) et retourne :
            - Le nouvel √©tat.
            - Une r√©compense associ√©e √† l‚Äôaction.
            - Un indicateur bool√©en pr√©cisant si la partie est termin√©e.
            - Des informations suppl√©mentaires utiles pour le d√©bogage ou l‚Äôanalyse.
        - `render()`: Affiche l‚Äô√©tat actuel du plateau.

    - Modes et fonctionnalit√©s sp√©cifiques
        - Mode Entrainement 
            - Utilis√© pour entra√Æner des agents en apprentissage par renforcement (RL) avec des algorithmes tels que PPO (Proximal Policy Optimization).
            - Interaction continue avec Stable-Baselines3, o√π env.py agit comme un pont entre l‚Äôalgorithme et le jeu.

        - Mode Interface
            - Permet de jouer directement via une interface, que ce soit entre humains ou contre des agents.
            - Gestion des actions non autoris√©es : Si un agent propose une action invalide (par exemple, d√©placer un pion qui ne peut pas bouger), une fonction de `reward.py` corrige cette action en la rempla√ßant par une action autoris√©e.
            - La correction suit un ordre par d√©faut, bas√© sur le type d'agent.

        - Mode Statistiques
            - Con√ßu pour analyser les performances des agents en simulant des parties compl√®tes.
            - Deux informations cl√©s sont enregistr√©es pour chaque action :
                - Si l‚Äôaction initialement propos√©e est valide.
                - L‚Äôaction r√©ellement ex√©cut√©e (apr√®s correction, si n√©cessaire).
            - Cela permet d‚Äô√©valuer non seulement les performances des agents, mais aussi leur capacit√© √† proposer des actions conformes aux r√®gles.

- `game_logic.py`: Contient l'impl√©mentation des r√®gles du jeu, la logique du jeu. G√®re les actions ainsi que leurs cons√©quences, v√©rifie quelles actions sont autoris√©es √† un moment donn√©... G√®re les validations des mouvements (d√©placement autoris√© ou non), les captures de pions, et la d√©tection des conditions de victoire.
- `renderer.py`: Responsable de l'affichage du jeu.
- `reward.py`: Impl√©mente les fonctions de r√©compense pour guider l‚Äôapprentissage des agents.
Les r√©compenses peuvent √™tre bas√©es sur :
La progression des pions sur le plateau.
La capture d‚Äôun pion adverse.
L‚Äôatteinte de la zone d‚Äôarriv√©e.
- `state.py`: D√©finit les √©tats dans lesquels peuvent se trouver les pions.

### `db/`

Le dossier `db/` regroupe tous les √©l√©ments n√©cessaires pour g√©rer la base de donn√©es associ√©e au projet, les scripts pour collecter, transformer et analyser les donn√©es, ainsi que les donn√©es utilis√©es pour nos analyses.

```bash
db/
‚îú‚îÄ‚îÄ analyse/
‚îÇ   ‚îú‚îÄ‚îÄ analyse_agents.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ analyse_entra√Ænement.ipynb
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ action_stats.csv
‚îÇ   ‚îú‚îÄ‚îÄ game.csv
‚îÇ   ‚îú‚îÄ‚îÄ game_rule.csv
‚îÇ   ‚îú‚îÄ‚îÄ is_rule_of.csv
‚îÇ   ‚îú‚îÄ‚îÄ participant.csv
‚îÇ   ‚îú‚îÄ‚îÄ player.csv
‚îÇ   ‚îî‚îÄ‚îÄ set_of_rules.csv
‚îú‚îÄ‚îÄ secret/                     # Dossier √† cr√©er en local
‚îÇ   ‚îî‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ config_game.py
‚îú‚îÄ‚îÄ export.py
‚îú‚îÄ‚îÄ insert.py
‚îú‚îÄ‚îÄ ludo_stats_play.py
‚îú‚îÄ‚îÄ notes_db.md
‚îú‚îÄ‚îÄ rules.py
‚îî‚îÄ‚îÄ schema.py
               
```

- `analyse/` : Dossier contenant les notebooks d'analyse des agents
    - `analyse_agents.ipynb` : Notebook d'analyse de l'entrainement des agents
    - `analyse_entra√Ænement.ipynb` : # TODO Daniil
- `data/` : Dossier contenant les fichiers csv de donn√©es export√©es depuis la base de donn√©es  et utilis√©s pour les analyses.
- `secret/config.py` : Fichier de configuration contenant l'URL de connexion √† la base de donn√©es PostgreSQL. Ce dossier est √† cr√©er localement.
- `config_game.py` : Fichier contenant les fonctions n√©cessaires pour g√©n√©rer les configurations de jeu des parties entre agents.
- `export.py`: Script permettant d'exporter les donn√©es de la base de donn√©es au format CSV.
- `insert.py` : Fichier contenant les classes permettant d'ins√©rer les donn√©es dans la base de donn√©es.
- `ludo_stats_play.py` : Scripts utilis√©s pour simuler les parties entre agents et enregistrer les donn√©es li√©es dans la base de donn√©es.  
Ce fichier contient plusieurs fonctions mains que nous avons utilis√©es selon nos besoins :
    - `main()` : Permet de lancer une ou plusieurs parties en d√©finissant depuis le terminal : le nombre de joueurs, le nombre de pions, la configuration de r√®gles, les agents a utiliser et le nombre de parties √† lancer.
    - `main_auto()` : Permet de lancer automatiquement plusieurs parties entre des agents identiques (m√™me type et m√™me nombre de pas d'entrainement).  
    Il faut pr√©ciser la configuration de r√®gles, le nombre de joueurs, le nombre de chevaux et le nombre de parties √† lancer.  
    Lance toutes les parties pour tous les agents d√©finis correspondant au nombre de joueurs, de pions et √† la configuration sp√©cifi√©e. 
    - `main_lancer_parties_pour_analyse_entrainement()` : Permet d'ex√©cuter les parties g√©n√©rant les donn√©es n√©cessaires √† l'analyse de l'entrainement des agents.
    - `MAIN_DANIIL`: TODO Daniil : Expliquer les fonctions que tu as utilis√© ??
- `db_configuration_and_setup.md` : Fichier fournissant les informations pour configurer et utiliser la base de donn√©es *ludo_stats*.
- `rules.py` : Fichier permettant de g√©rer les r√®gles (d√©finition, description et d√©termination dynamique).
- `schema.py` : Script permettant d'initialiser la base de donn√©es en cr√©ant les tables n√©cessaires.

### analyse 

TODO DANIIL ICI D√©crit √† quels endroits sont les notebooks que le prof doit regarder pour les stats 



--- 
TODO 
√† supprimer : voil√† l'√©nonc√© : 

Le code doit inclure au minimum un README.txt (ou mieux un README.md) avec des explications. Le README contiendra les informations suivantes :

- Objectifs : ce que fait le projet, une description des diff√©rentes fonctionnalit√©s disponibles.

- Installation : comment le tester/compiler, d√©pendances. Le projet devra √™tre compilable/utilisable par vos √©valuateurs.

- Organisation et explications du code, explication de ce que font chaque ex√©cutable/parties des donn√©es : comment les r√©cup√©rer, etc.

- R√©sultats et analyses.

- Des m√©dias (images, vid√©os d'explications) pourront √™tre fournis pour indiquer comment correctement utiliser l'application.