<div align="right">
  <a href="README.md">English</a> |
  <b>FranÃ§ais</b>
</div>

# DÃ©veloppement dâ€™agents autonomes et crÃ©ation de nouvelles rÃ¨gles pour jeux de plateau via lâ€™apprentissage par renforcement
[![License: MIT](https://img.shields.io/badge/License-MIT-lightgrey.svg)](https://opensource.org/licenses/MIT)

## Description du Projet ğŸ®ğŸ¤–

Ce projet explore l'apprentissage par renforcement appliquÃ© Ã  des jeux de plateau, avec un focus sur le cÃ©lÃ¨bre jeu de sociÃ©tÃ© Ludo (Ã©galement connu sous le nom de "Petits Chevaux"). Initialement, nous avions expÃ©rimentÃ© avec le jeu Labyrinthe, mais ce choix a Ã©tÃ© abandonnÃ© en raison de contraintes spÃ©cifiques, comme expliquÃ© dans la documentation.

---

### Description reformulÃ©e du sujet et les objectifs du projet

Ce projet vise Ã  dÃ©velopper diffÃ©rents agents autonomes pour des jeux de plateau en sâ€™appuyant sur des
techniques dâ€™apprentissage par renforcement. Lâ€™objectif principal est double : dâ€™une part, entraÃ®ner ces
agents Ã  adopter des stratÃ©gies optimales adaptÃ©es aux rÃ¨gles du jeu, et dâ€™autre part, Ã©tudier les interactions
entre diffÃ©rentes rÃ¨gles et diffÃ©rents types dâ€™agents afin dâ€™analyser leur influence sur la jouabilitÃ© et lâ€™Ã©quilibre
global du jeu.

1. ModÃ©lisation du jeu
    - ReprÃ©senter le jeu de maniÃ¨re formelle, avec ses Ã©tats, actions et espaces dâ€™observation.
    - IntÃ©grer dÃ¨s le dÃ©part la possibilitÃ© dâ€™ajouter des rÃ¨gles ou variantes futures, en prÃ©voyant une
architecture flexible.

2. Apprentissage par renforcement
    - Utiliser des techniques de RL pour entraÃ®ner des agents autonomes capables de jouer effica-
cement au jeu choisi.
3. Comportements et personnalisation des agents
    - DÃ©velopper diffÃ©rents types dâ€™agents, chacun ayant des stratÃ©gies distinctes.
    - Ã‰tudier leurs comportements face aux variantes des rÃ¨gles testÃ©es.
4. Ajout de nouvelles rÃ¨gles et variantes
    - Analyser comment ces variantes influencent les comportements des agents.
    - Tester lâ€™impact de diffÃ©rentes rÃ¨gles sur la jouabilitÃ© et lâ€™Ã©quilibre du jeu.
5. Analyse statistique des parties simulÃ©es
    - Collecter des statistiques dÃ©taillÃ©es sur les parties jouÃ©es par les agents pour Ã©valuer leurs
performances.
    - DÃ©finir des critÃ¨res pour comparer les rÃ¨gles ou les variantes afin de dÃ©terminer ce qui amÃ©liore
le gameplay.

## FonctionnalitÃ©s principales :
ğŸ§  CrÃ©ation d'agents : Plusieurs agents ont Ã©tÃ© dÃ©veloppÃ©s, utilisant notamment l'algorithme Proximal Policy Optimization (PPO) pour optimiser leurs stratÃ©gies.  
âš™ï¸ EntraÃ®nement des agents : Les agents ont Ã©tÃ© entraÃ®nÃ©s sur des environnements simulÃ©s, avec des rÃ¨gles variÃ©es pour modÃ©liser diffÃ©rents scÃ©narios de jeu.  
ğŸ² Simulation de parties : Nous avons simulÃ© des affrontements entre agents pour analyser leurs performances dans diffÃ©rents contextes, tout en testant les impacts des variations de rÃ¨gles.  
ğŸ“Š Analyse des performances : Une analyse approfondie des rÃ©sultats a Ã©tÃ© rÃ©alisÃ©e Ã  l'aide de techniques statistiques et des outils dÃ©diÃ©s. 
ğŸ® Interface graphique interactive : Lâ€™interface graphique dÃ©veloppÃ©e permet Ã  lâ€™utilisateur humain dâ€™affronter les agents directement ou de simuler des parties entre humains.

---

## RÃ¨gles du Jeu et Variations ğŸ“ğŸ²

### RÃ¨gles de Base :

- Chaque joueur commence avec tous ses pions dans son Ã©curie.
- Un 6 au dÃ© est requis pour sortir un pion de l'Ã©curie.
- Une fois sur le plateau, les pions avancent sur un chemin commun de 56 cases menant Ã  un escalier de 6 cases, propre Ã  chaque joueur, qui aboutit Ã  une case objectif.
- RÃ¨gles pour les dÃ©placements dans le chemin commun :
    - Tuer un pion adverse : Un pion peut Ã©liminer un pion adverse uniquement sâ€™il tombe exactement sur la mÃªme case.
    - Rejoindre un pion alliÃ© : Un pion dâ€™un joueur peut rejoindre un autre pion du mÃªme joueur uniquement si le rÃ©sultat du lancer de dÃ© correspond exactement Ã  la distance qui les sÃ©pare.
    - Rester bloquÃ© derriÃ¨re un pion : Si la valeur du dÃ© est supÃ©rieure au nombre de cases jusquâ€™au pion suivant sur le plateau (quâ€™il appartienne au mÃªme joueur ou Ã  un adversaire), le pion avancera jusquâ€™Ã  la case prÃ©cÃ©dant lâ€™obstacle. Les dÃ©passements de pions sont interdits. Ces rÃ¨gles ne sâ€™appliquent pas Ã  lâ€™escalier.
- **Disposition des Ã©curies selon le nombre de joueurs** :
    - **2 joueurs** : Les Ã©curies sont placÃ©es Ã  l'opposÃ© l'une de l'autre sur le plateau. Ainsi, la case 1 du chemin pour un joueur correspond Ã  la case 29 pour l'autre.
    - **3 ou 4 joueurs** : Les Ã©curies sont rÃ©parties de maniÃ¨re Ã©quidistante toutes les 14 cases. Une mÃªme case peut Ãªtre perÃ§ue diffÃ©remment selon le point de vue du joueur :
        Par exemple, la case 1 pour un joueur sera la case 15, case 29 ou case 43 pour les autres joueurs, en fonction de leur position de dÃ©part.
        
    Cela garantit une rÃ©partition Ã©quilibrÃ©e des positions de dÃ©part sur le plateau.

### Variations des RÃ¨gles :
- Nombre de joueurs :
    - Le jeu peut Ãªtre jouÃ© Ã  2, 3 ou 4 joueurs.
- Nombre de pions par joueur :
    - Chaque joueur peut avoir entre 2 et 6 petits chevaux en jeu.
- Conditions de victoire :
    - Victoire rapide : Le premier joueur Ã  atteindre lâ€™objectif avec un seul pion gagne.
    - Victoire complÃ¨te : Tous les pions dâ€™un joueur doivent atteindre lâ€™objectif pour dÃ©clarer sa victoire.
- RÃ¨gles pour atteindre le pied de l'escalier :
    - Exactitude nÃ©cessaire : Un pion doit atteindre exactement la case situÃ©e au pied de l'escalier pour commencer Ã  le gravir.
        - Si le lancÃ© de dÃ© dÃ©passe la distance requise pour atteindre cette case, le pion peut avancer puis reculer, Ã  condition que ce mouvement rÃ©duise la distance qui le sÃ©pare de la case au pied de l'escalier.
        - Si, en avanÃ§ant puis reculant selon la valeur du dÃ©, le pion finit par s'Ã©loigner davantage de la case pied de l'escalier, alors il ne peut pas Ãªtre dÃ©placÃ©.
    - Progression simplifiÃ©e : Si la valeur du dÃ© dÃ©passe le pied de lâ€™escalier, le pion grimpe directement comme si lâ€™escalier faisait partie du chemin.
- Ordre de progression sur lâ€™escalier :
    - Ordre simplifiÃ© : Un pion avance de la distance indiquÃ©e par le dÃ©. Cela lui permet de monter plusieurs marches de lâ€™escalier en un seul lancer, et dâ€™atteindre directement la case objectif si la valeur du dÃ© le permet ou la dÃ©passe.
    - Ordre strict (autorisÃ© uniquement si lâ€™exactitude est requise pour atteindre le pied de lâ€™escalier):
        - Chaque marche de lâ€™escalier nÃ©cessite un lancer spÃ©cifique : 1 pour la premiÃ¨re marche, 2 pour la deuxiÃ¨me, ..., et 6 pour atteindre la case objectif.
        - Une rÃ¨gle optionnelle peut Ãªtre ajoutÃ©e pour permettre ou non au pion de rejouer aprÃ¨s chaque montÃ©e de marche.
- Rejouer en cas de lancer dâ€™un 6 : Cette rÃ¨gle peut Ãªtre activÃ©e ou dÃ©sactivÃ©e.
- Protection des pions : Si deux pions dâ€™un mÃªme joueur se trouvent sur la mÃªme case, ils deviennent invulnÃ©rables et ne peuvent pas Ãªtre Ã©liminÃ©s. Cette rÃ¨gle peut Ãªtre activÃ©e ou dÃ©sactivÃ©e.

## DiffÃ©rents agents : 

Dans notre projet, nous avons dÃ©veloppÃ© six types d'agents distincts, chacun avec sa propre stratÃ©gie et son systÃ¨me de rÃ©compenses :

- **Agent Balanced (Ã‰quilibrÃ©)** : Agent utilisant une stratÃ©gie Ã©quilibrÃ©e entre attaque et progression, servant de rÃ©fÃ©rence pour comparer les autres agents.

- **Agent Aggressive (Agressif)** : Agent privilÃ©giant les actions offensives avec des rÃ©compenses maximales pour l'Ã©limination des pions adverses.

- **Agent Rusher (PressÃ©)** : Agent focalisÃ© sur la progression rapide vers l'objectif avec un systÃ¨me de rÃ©compenses favorisant les mouvements vers l'avant et l'atteinte du but.

- **Agent Defensive (DÃ©fensif)** : Agent priorisant la sÃ©curitÃ© des pions avec des rÃ©compenses importantes pour l'atteinte des zones sÃ»res et la protection mutuelle des pions.

- **Agent Spawner (Sortie rapide)** : Agent concentrÃ© sur la sortie rapide des pions de l'Ã©curie avec des rÃ©compenses maximales pour les actions de sortie.

- **Agent Suboptimal (Sous-optimal)** : Agent simulant un joueur inexpÃ©rimentÃ© avec des rÃ©compenses plus Ã©levÃ©es pour les actions gÃ©nÃ©ralement moins efficaces.

Chaque agent dispose de sa propre table de rÃ©compenses, adaptÃ©e selon sa stratÃ©gie. Cette diversitÃ© permet d'explorer diffÃ©rentes approches du jeu et d'Ã©tudier leur efficacitÃ© relative dans diffÃ©rentes configurations de rÃ¨gles.

### Mindmap rÃ©sumant les diffÃ©rentes configurations

![Mindmap des rÃ¨gles du jeu](/docs/mindmap_setup.png)

---

## Technologies utilisÃ©es :
ğŸ Python : Langage principal pour la gestion du jeu et des agents.  
ğŸ› ï¸ Gymnasium : Environnements personnalisÃ©s pour l'apprentissage par renforcement.  
ğŸ¤– Stable-Baselines3 : BibliothÃ¨que utilisÃ©e pour entraÃ®ner les agents sur les environnements Gymnasium.  
ğŸ—„ï¸ PostgreSQL : Base de donnÃ©es pour stocker les rÃ©sultats des simulations et les mÃ©triques des agents.  
ğŸ“Š Pandas et Jupyter Notebook : Analyse et visualisation des performances des agents.  
ğŸ¨ Pygame : Interface graphique pour visualiser les parties en temps rÃ©el.  
âœ… Pytest : Tests unitaires pour garantir la fiabilitÃ© du code.  

--- 

## Installation 

#### Version de Python 
Nous avons utilisÃ© **Python 3.11** pour ce projet.
Assurez-vous que cette version est installÃ©e sur votre machine : 

##### Installation de Python 3.11
- MacOS (via Homebrew) : 
```bash
brew install python@3.11
```

- Linux (Ubuntu):
```bash
sudo apt update
sudo apt install python3.11
```

##### VÃ©rification de l'installation : 
```bash
python3.11 --version
```

#### CrÃ©ation de l'environnement virtuel
Pour garantir la compatibilitÃ© et faciliter les tests, nous avons utilisÃ© `venv`.

##### VÃ©rifier la disponibilitÃ© de `venv``
```bash
python3.11 -m venv --help
```
- Si cette commande fonctionne, vous pouvez continuer.
- Sinon installez `venv`:
    - Sous MacOS, lorsque Python 3.11 est installÃ© via Homebrew, le module `venv` est inclus par dÃ©faut. Si la commande prÃ©cÃ©dent a gÃ©nÃ©rÃ© une erreur : 
    ```bash
    brew update
    brew upgrade python
    brew reinstall python@3.11
    ```

    - Linux (Ubuntu)
    ```bash
    sudo apt update
    sudo apt install python3.11-venv
    ```


##### Ã‰tapes pour crÃ©er et configurer l'environnement virtuel 
- CrÃ©er l'environnement virtuel
Depuis la racine du projet : 
```bash
python3.11 -m venv ludo_venv
```

- Activer l'environnement virtuel 
    ```bash
    source ludo_venv/bin/activate
    ```

- Mettre Ã  jour `pip` dans l'environnement virtuel
```bash
pip install --upgrade pip
```

- Installer les dÃ©pendances 
```bash
pip install -r requirements_venv.txt
```

##### DÃ©sactiver l'environnement virtuel
Une fois que vous avez terminÃ© vos tests ou que vous n'avez plus besoin d'utiliser l'environnement virtuel, vous pouvez le dÃ©sactiver facilement. Cela vous permettra de revenir Ã  votre environnement Python global ou de systÃ¨me.

Pour dÃ©sactiver l'environnement virtuel, exÃ©cutez simplement la commande suivante :
```bash
deactivate
```

Cela dÃ©sactive l'environnement virtuel actif sans supprimer ses fichiers. Vous pourrez le rÃ©activer ultÃ©rieurement si nÃ©cessaire.

### Utilisation de l'environnement virtuel dans les notebooks 

Pour les analyses, expÃ©rimentations et entraÃ®nements, nous avons utilisÃ© des notebooks Jupyter via VSCode. Si vous souhaitez exÃ©cuter un notebook dans le cadre de ce projet, nous vous recommandons dâ€™utiliser VSCode, car nous n'avons pas testÃ© cette configuration avec d'autres outils ou Ã©diteurs.

Le package `ipykernel`, nÃ©cessaire pour connecter l'environnement virtuel aux notebooks Jupyter, est dÃ©jÃ  inclus dans les dÃ©pendances listÃ©es dans le fichier `requirements_venv.txt`.

##### Ã‰tapes pour configurer le kernel dans VSCode : 

- Ouvrez un notebook `.ipynb` dans VSCode
- Cliquez sur **Run All** ou sur l'option **Select Kernel** situÃ©e en haut Ã  droite de l'interface.
- Dans le menu qui s'affiche, cliquez sur **Select Another Kernel...**
- Dans la section **Python Envrionments**, choisissez l'environnement virtuel correspondant (`ludo_venv`)

Une fois ces Ã©tapes terminÃ©es, le notebook sera configurÃ© pour utiliser l'environnement virtuel, et vous pourrez exÃ©cuter vos analyses en toute compatibilitÃ© avec les dÃ©pendances du projet.

### Ã‰tapes pour configurer la base de donnÃ©es

Veuillez vous rÃ©fÃ©rer au fichier suivant pour les instructions complÃ¨tes sur la configuration de PostgreSQL, la crÃ©ation de la base de donnÃ©es ludo_stats, et l'initialisation de sa structure : `./db/Windows_db_configuration_and_setup.md` et `./db/Ubuntu_db_configuration_and_setup.md`

Ces instructions sont spÃ©cifiquement adaptÃ©es pour Windows et Ubuntu. Si vous utilisez un autre systÃ¨me d'exploitation, elles peuvent ne pas fonctionner. Dans ce cas, veuillez chercher sur internet comment installer PostgreSQL pour votre environnement.

Notez que **l'installation de PostgreSQL n'est pas obligatoire**, sauf si vous souhaitez enregistrer de nouvelles donnÃ©es. Les donnÃ©es nÃ©cessaires pour nos analyses ont dÃ©jÃ  Ã©tÃ© exportÃ©es en fichiers CSV et se trouvent dans le dossier `db/data`.

---

## Lancer une partie avec interface graphique
Pour jouer avec l'interface graphique, placer vous dans le dossier `game`

```bash
cd game
```

Puis exÃ©cuter le fichier `play.py` comme ceci:

```bash
python3 play_pygame/play.py
```

Voici un extrait d'une partie montrant un joueur humain et trois agents en action.
![Demo of the app](docs/demo_assets/demo_pc_1humain_3agents.gif)

#### Agents Non EntraÃ®nÃ©s
AprÃ¨s avoir configurÃ© les rÃ¨gles du jeu selon vos prÃ©fÃ©rences, il est possible que l'agent correspondant Ã  cette configuration ne soit pas encore entraÃ®nÃ©. Si un agent non entraÃ®nÃ© est utilisÃ©, le programme Ã©chouera.
Dans ce cas, il faut relancer le programme et choisir une configuration de rÃ¨gles diffÃ©rente.

**Message d'Erreur Attendu**

Voici un exemple de message d'erreur qui peut apparaÃ®tre dans ce cas :
```bash
"Le fichier <model_file_name> n'existe pas. Veuillez entraÃ®ner l'agent avant de l'utiliser."
```
---

## Tests avec Pytest

Afin de garantir que la logique du jeu est robuste et fonctionne comme prÃ©vu, nous avons mis en place des tests unitaires avec Pytest. Ces tests couvrent diffÃ©rents aspects de la logique du jeu pour s'assurer que chaque fonctionnalitÃ© est correctement implÃ©mentÃ©e.

#### Lancer les tests Pytest 
Pour exÃ©cuter les tests, utilisez la commande suivante Ã  la racine du projet :
```bash
pytest game/tests_pytest/
```

#### RÃ©sultat attendu
Si tous les tests passent avec succÃ¨s, vous devriez voir une sortie similaire Ã  celle-ci :

```bash
============================== 81 passed, 1 warning in 1.30s ==============================
```

Cela indique que 81 tests ont Ã©tÃ© validÃ©s avec succÃ¨s. Le warning peut Ãªtre dÃ» Ã  une dÃ©pendance ou une configuration et ne devrait pas affecter le fonctionnement principal du jeu.

---


## RÃ©sultats et analyses

### Base de donnÃ©es et simulations de parties

#### Base de donnÃ©es

Le projet sâ€™appuie sur une base de donnÃ©es relationnelle PostgreSQL pour collecter, structurer et analyser les donnÃ©es gÃ©nÃ©rÃ©es lors des simulations de parties. Cette base de donnÃ©es centralise des informations pertinentes sur les joueurs, les parties, les rÃ¨gles et les actions effectuÃ©es.

![SchÃ©ma de la base de donnÃ©es](docs/schema_db.jpg)

#### Simulations de parties

Les simulations de parties sont exÃ©cutÃ©es de maniÃ¨re automatisÃ©e grÃ¢ce au fichier `ludo_stats_play.py` depuis le dossier `./db`. Ce script est conÃ§u pour lancer un grand nombre de parties entre agents, avec pour objectif principal dâ€™enregistrer les donnÃ©es gÃ©nÃ©rÃ©es lors de chaque partie. Les statistiques collectÃ©es, telles que les performances des agents, leurs actions, leurs scores, et les rÃ©sultats des parties, sont automatiquement sauvegardÃ©es dans la base de donnÃ©es. Ces informations servent ensuite Ã  l'analyse des performances et des comportements des agents.

Les donnÃ©es gÃ©nÃ©rÃ©es lors des simulations sont enregistrÃ©es dans la base de donnÃ©es, puis exportÃ©es au format CSV pour faciliter leur analyse dans des notebooks Python. Ces fichiers constituent la base dâ€™analyses dÃ©taillÃ©es, permettant de visualiser des mÃ©triques clÃ©s telles que les scores obtenus, les taux dâ€™erreurs, ou encore la rÃ©partition des types dâ€™actions effectuÃ©es par les agents. Ces analyses visent Ã  dÃ©montrer lâ€™efficacitÃ© de lâ€™entraÃ®nement des agents, leurs performances, mais aussi Ã  identifier les marges dâ€™amÃ©lioration dans leurs comportements et stratÃ©gies.

### Analyse de l'entrainement des agents 

NB : L'analyse complÃ¨te de l'entrainement est disponible dans le notebook `db/analyse/analyse_entrainement.ipynb`.

#### Objectif

Ã‰tudier l'entrainement des agents RL en analysant l'Ã©volution de leurs performances en fonction du nombre de pas d'entraÃ®nement, Ã  travers des configurations de jeu de complexitÃ© croissante.

#### DonnÃ©es et paramÃ¨tres de l'analyse
**DonnÃ©es**  

Les donnÃ©es utilisÃ©es pour cette analyse ont Ã©tÃ© collectÃ©es Ã  partir de simulations massives de parties entre
agents identiques, câ€™est-Ã -dire de mÃªme type et de mÃªme nombre de pas dâ€™entraÃ®nement, lancÃ©es avec la fonction `main_lancer_parties_pour_analyse_entrainement()` du fichier `ludo_stats_play.py`.

**MÃ©triques**  
- RÃ©partition des types d'actions demandÃ©es
- Pourcentage moyen d'erreurs
- Score moyen des agents

**Configurations**  
- Configuration 16 : RÃ¨gles de base sans contraintes
- Configuration 12 : RÃ¨gles intermÃ©diaires avec contraintes
- Configuration 17 : RÃ¨gles complÃ¨tes avec interactions avancÃ©es

**Nombre et types d'agents**  
Tous les types agents ont Ã©tÃ© Ã©tudiÃ©s (Balanced, Aggressive, Rusher, Defensive, Spawner, et Suboptimal) dans des parties avec : 2 joueurs et 2 pions, 2 joueurs et 4 pions, et 4 joueurs et 4 pions. Nous avons entrainÃ©, fait jouer, et analysÃ© les performances de ces agents avec les nombres pas d'entrainement suivants : 50 000, 100 000, 200 000, et 400 000.

#### RÃ©sultats

Les rÃ©sultats ont montrÃ©s que d'une maniÃ¨re gÃ©nÃ©rale, l'entraÃ®nement des agents permet l'apprentissage progressif de nouvelles actions, confirmant que le processus d'entraÃ®nement fonctionne. Les agents apprennent Ã  s'adapter Ã  leur environnement et Ã  maÃ®triser les rÃ¨gles associÃ©es.

Pour la **configuration 16**, la plus simple, les agents montrent des performances claires et stables. Le pourcentage moyen d'erreurs diminue, et les scores augmentent au cours de l'entrainement, indiquant une bonne capacitÃ© d'adaptation Ã  l'environnement. En revanche, l'augmentation du nombre de joueurs et de pions provoque un ralentissement de l'entrainement. 

Exemple pour l'agent aggressive :
![16-2-2-aggressive](/docs/graphiques_analyse_entrainement/16-2-2-aggressive.png)
![16-2-4-aggressive](/docs/graphiques_analyse_entrainement/16-2-4-aggressive.png)
![16-4-4-aggressive](/docs/graphiques_analyse_entrainement/16-4-4-aggressive.png)

Avec l'introduction de rÃ¨gles supplÃ©mentaires, les performances des agents deviennent moins stables. Bien que certains agents continuent de progresser, d'autres montrent des fluctuations dans leurs taux d'erreurs et leurs scores. 

Pour la **configuration 12**, l'entraÃ®nement reste efficace, mais il est moins concluant que pour la configuration 16. Les agents apprennent certaines actions adaptÃ©es aux nouvelles rÃ¨gles, mais leur capacitÃ© d'adaptation est moins marquÃ©e, et les performances globales restent limitÃ©es.

Exemple de l'agent aggressive :
![12-2-2-aggressive](/docs/graphiques_analyse_entrainement/12-2-2-aggressive.png)

- Actions 50000 pas : NO_ACTION
- Nouvelles actions 100000 : MOVE_FORWARD, MARCHE_2
- Nouvelles actions 200000 : KILL
- Nouvelles actions 400000 : AVANCE_RECULE_PIED_ESCALIER

Exemple de diffÃ©rence entre les agents aggressive et balanced :
![12-4-4-aggressive](/docs/graphiques_analyse_entrainement/12-4-4-aggressive.png)
![12-4-4-balanced](/docs/graphiques_analyse_entrainement/12-4-4-balanced.png)

Enfin, dans la **configuration 17**, la plus complexe, les agents rencontrent les plus grandes difficultÃ©s. Le pourcentage moyen d'erreurs reste Ã©levÃ©, et les scores moyens restent trÃ¨s faibles, montrant une incapacitÃ© des agents Ã  apprendre les rÃ¨gles du jeu. De plus, l'apprentissage de nouvelles actions est plus lent.

Exemple de l'agent aggressive :
![17-2-2-aggressive](/docs/graphiques_analyse_entrainement/17-2-2-aggressive.png)

- Actions 50000 pas : NO_ACTION
- Nouvelles actions 100000 : MOVE_FORWARD
- Nouvelles actions 200000 : MARCHE_6
- Nouvelles actions 400000 : MOVE_OUT


Ces rÃ©sultats montrent que les agents s'entrainent bien pour des configurations de rÃ¨gles simples, mais mettent cependant en Ã©vidence les limites des agents RL Ã  s'adapter Ã  des environnements de plus en plus complexes.

### Analyse des performances des agents

L'analyse complÃ¨te des performances des agents est disponible dans le notebook `db/analyse/analyse_agents.ipynb`. Nous prÃ©sentons ici une synthÃ¨se des rÃ©sultats principaux.

L'Ã©tude des performances des agents dans notre jeu reprÃ©sente un dÃ©fi analytique important en raison du nombre considÃ©rable de configurations possibles : 32 configurations de rÃ¨gles diffÃ©rentes, parties Ã  2 ou 4 joueurs, 6 types d'agents, et possibilitÃ©s de 2 ou 4 pions par joueur. Nous nous sommes donc concentrÃ©s sur les configurations les plus reprÃ©sentatives pour illustrer nos analyses.

#### Performance des agents au sein d'une configuration fixe

Pour la configuration 8, qui offre un Ã©quilibre intÃ©ressant entre les diffÃ©rentes stratÃ©gies, nous avons analysÃ© les performances relatives des agents.

![Matrice des taux de victoire entre agents](/docs/graphiques_analyse_agents/taux_victoire_conf8.png)

*Figure 1: Matrice des taux de victoire entre agents (configuration 8)*

Cette matrice rÃ©vÃ¨le des diffÃ©rences significatives dans l'efficacitÃ© des stratÃ©gies. Par exemple, l'agent aggressive montre une forte domination contre l'agent defensive (62% de victoires) mais peine davantage face Ã  l'agent rusher (49% de victoires).

Dans les parties Ã  quatre joueurs, la dynamique change significativement :

![Distribution des positions finales](/docs/graphiques_analyse_agents/distribution_places_conf8.png)

*Figure 2: Distribution des positions finales par type d'agent en parties Ã  4 joueurs*

Face Ã  des adversaires multiples, les Ã©carts de performance se rÃ©duisent. L'agent defensive atteint la premiÃ¨re place aussi souvent que l'agent aggressive, mais montre des performances moindres pour les deuxiÃ¨mes places.

#### Satisfaction des agents selon les rÃ¨gles

La satisfaction des agents a Ã©tÃ© Ã©valuÃ©e selon des mÃ©triques spÃ©cifiques Ã  chaque stratÃ©gie. Pour l'agent aggressive par exemple :

![Satisfaction de l'agent aggressive](/docs/graphiques_analyse_agents/satisfaction_aggressive.png)

*Figure 3: Nombre moyen d'Ã©liminations par partie selon les configurations*

Les configurations 11 et 12, avec leurs contraintes plus strictes sur la progression vers l'objectif, permettent plus d'Ã©liminations et donc une meilleure satisfaction pour l'agent aggressive. Cependant, ce taux de satisfaction Ã©levÃ© ne correspond pas toujours Ã  un meilleur taux de victoire.

#### Ã‰quilibre et jouabilitÃ©

L'analyse de la configuration 7 rÃ©vÃ¨le des aspects intÃ©ressants sur l'Ã©quilibre du jeu :

![Longueur des parties](/docs/graphiques_analyse_agents/longueur_parties_conf7.png)

*Figure 4: Distribution des longueurs de parties selon les types d'agents*

La durÃ©e des parties reste remarquablement stable quelle que soit la combinaison d'agents, suggÃ©rant un bon Ã©quilibre gÃ©nÃ©ral.

Cependant, l'analyse des parties 2v2 montre une autre rÃ©alitÃ© :

![Distribution des pions en 2v2](/docs/graphiques_analyse_agents/pions_objectif_2v2.png)

*Figure 5: Distribution du nombre de pions Ã  l'objectif par Ã©quipe*

La dispersion des points rÃ©vÃ¨le que certaines combinaisons d'agents sont nettement plus efficaces que d'autres pour atteindre l'objectif, indiquant des dÃ©sÃ©quilibres dans cette configuration.


---

## Arborescence du projet

### Ã€ la racine 

Voici la structure des principaux dossiers et fichiers de ce projet, avec une description de leur contenu et rÃ´le.

```bash
.
â”œâ”€â”€ README.md
â”œâ”€â”€ README.fr.md
â”œâ”€â”€ docs/
â”œâ”€â”€ db/
â”œâ”€â”€ game/
â”œâ”€â”€ resources/
â””â”€â”€ requirements_venv.txt
```

#### Description des dossiers et fichiers

- `README.md` : Contient la documentation principale du projet, y compris les objectifs, instructions d'installation et exemples d'utilisation en anglais.
- `README.fr.md`: Contient la documentation principale du projet, y compris les objectifs, instructions d'installation et exemples d'utilisation en franÃ§ais.
- `docs/` : UtilisÃ© tout au long du semestre pour centraliser les recherches, notes, explications, choix d'Ã©quipe et toute trace Ã©crite utile Ã  communiquer.
Contient Ã©galement les fichiers de documentation complÃ©mentaires, tels que :
    - Des explications techniques sur le projet.
    - Des captures d'Ã©cran ou diagrammes pour illustrer les concepts clÃ©s.
- `db/` : Ce dossier stocke les fichiers nÃ©cessaires Ã  la gestion et Ã  lâ€™exploitation des donnÃ©es du projet. Il contient :
    - Les scripts pour initialiser la base de donnÃ©es, insÃ©rer des donnÃ©es, et les exporter au format CSV.
    - Les donnÃ©es exportÃ©es utilisÃ©es pour les analyses.
    - Des notebooks pour analyser les performances et lâ€™entraÃ®nement des agents.
    - Les fichiers de configuration de la base PostgreSQL.
    - Des scripts pour gÃ©rer les rÃ¨gles du jeu et les configurations des parties simulÃ©es.
- `game/`: Ce dossier constitue le cÅ“ur du projet et contient :
    - La logique interne du jeu, ainsi que l'environnement Gym attachÃ© pour les simulations et l'apprentissage par renforcement.
    - Les fichiers nÃ©cessaires pour lancer le jeu avec une interface graphique.
    - Des notebooks dÃ©diÃ©s Ã  l'apprentissage automatique, permettant d'entraÃ®ner et d'Ã©valuer des agents.
    - Des tests en pytest pour garantir que la logique du jeu respecte les rÃ¨gles dÃ©finies.
- `resources/` : Contient les documents liÃ©s au projet :
    - `project-presentation-slideshow-fr.pdf`: Diaporama de prÃ©sentation du projet en franÃ§ais.
    - `project-report-fr.pdf`: Rapport complet du projet rÃ©digÃ© en franÃ§ais.
    - `project-assignment-fr.pdf`: Ã‰noncÃ© initial du projet en franÃ§ais.

- `requirements_venv.txt` : Une version spÃ©cifique des dÃ©pendances utilisÃ©e avec lâ€™environnement virtuel.

### `game/`

```bash
game/
â”œâ”€â”€ __init__.py                  
â”œâ”€â”€ images/                        
â”œâ”€â”€ ludo_env/                    
â”œâ”€â”€ play_pygame/    
â”œâ”€â”€ reinforcement_learning/        
â””â”€â”€ tests_pytest/                  
```

- `__init__.py` : Fichier d'initialisation pour le module Python.
- `images/` : Contient les images utilisÃ©es pour l'interface graphique.
-  `ludo_env/`: Ce rÃ©pertoire contient lâ€™implÃ©mentation complÃ¨te de lâ€™environnement Gym pour le jeu Ludo, incluant :
    - La logique du jeu.
    - La gestion des Ã©tats et actions.
    - L'intÃ©gration avec Gym pour permettre lâ€™entraÃ®nement dâ€™agents RL.
- `play_pygame/`: Dossier contenant le code pour jouer au jeu avec une interface graphique dÃ©veloppÃ©e avec Pygame.
- `reinforcement_learning/` : Inclut les notebooks et scripts relatifs Ã  l'apprentissage par renforcement.
- `tests_pytest/`: Contient les tests unitaires Ã©crits avec pytest pour s'assurer que :
    - Les rÃ¨gles du jeu sont correctement implÃ©mentÃ©es.
    - Les actions de lâ€™environnement respectent les contraintes dÃ©finies.
    - Les rÃ©sultats sont conformes aux attentes pour diffÃ©rents scÃ©narios.

### `game/ludo_env`

```bash
game/                     
â””â”€â”€ ludo_env/                    
    â”œâ”€â”€ __init__.py              
    â”œâ”€â”€ action.py                
    â”œâ”€â”€ env.py                   
    â”œâ”€â”€ game_logic.py            
    â”œâ”€â”€ renderer.py             
    â”œâ”€â”€ reward.py                
    â””â”€â”€ state.py                 
```

- `__init__.py` : Ce fichier fait de ludo_env un module Python. Il permet d'importer facilement les fichiers du rÃ©pertoire dans d'autres parties du projet.
- `action.py` : DÃ©finit les actions disponibles pour les agents dans le jeu.
- `env.py` : Lâ€™environnement Gym au cÅ“ur du projet
    -  Le fichier env.py est une composante centrale de notre implÃ©mentation. Il constitue une interface standardisÃ©e pour :
        - Jouer au jeu Ludo entre humains via une interface graphique ou textuelle.
        - Effectuer des entraÃ®nements en apprentissage par renforcement (RL).
        - Simuler des milliers de parties afin de collecter des donnÃ©es statistiques ou Ã©valuer les performances des agents.

    - Fonctions principales de env.py 
        - `reset()`: Initialise une nouvelle partie et met lâ€™environnement dans son Ã©tat de dÃ©part.
    Retourne lâ€™Ã©tat initial du plateau sous une forme exploitable par lâ€™agent RL ou par des simulations.
        - `step(action)`: ReÃ§oit une action (proposÃ©e par un agent ou un humain).
    ExÃ©cute cette action, calcule les consÃ©quences (rÃ©compense, Ã©tat suivant, fin de partie, etc.) et retourne :
            - Le nouvel Ã©tat.
            - Une rÃ©compense associÃ©e Ã  lâ€™action.
            - Un indicateur boolÃ©en prÃ©cisant si la partie est terminÃ©e.
            - Des informations supplÃ©mentaires utiles pour le dÃ©bogage ou lâ€™analyse.
        - `render()`: Affiche lâ€™Ã©tat actuel du plateau.

    - Modes et fonctionnalitÃ©s spÃ©cifiques
        - Mode Entrainement 
            - UtilisÃ© pour entraÃ®ner des agents en apprentissage par renforcement (RL) avec des algorithmes tels que PPO (Proximal Policy Optimization).
            - Interaction continue avec Stable-Baselines3, oÃ¹ env.py agit comme un pont entre lâ€™algorithme et le jeu.

        - Mode Interface
            - Permet de jouer directement via une interface, que ce soit entre humains ou contre des agents.
            - Gestion des actions non autorisÃ©es : Si un agent propose une action invalide (par exemple, dÃ©placer un pion qui ne peut pas bouger), une fonction de `reward.py` corrige cette action en la remplaÃ§ant par une action autorisÃ©e.
            - La correction suit un ordre par dÃ©faut, basÃ© sur le type d'agent.

        - Mode Statistiques
            - ConÃ§u pour analyser les performances des agents en simulant des parties complÃ¨tes.
            - Deux informations clÃ©s sont enregistrÃ©es pour chaque action :
                - Si lâ€™action initialement proposÃ©e est valide.
                - Lâ€™action rÃ©ellement exÃ©cutÃ©e (aprÃ¨s correction, si nÃ©cessaire).
            - Cela permet dâ€™Ã©valuer non seulement les performances des agents, mais aussi leur capacitÃ© Ã  proposer des actions conformes aux rÃ¨gles.

- `game_logic.py`: Contient l'implÃ©mentation des rÃ¨gles du jeu, la logique du jeu. GÃ¨re les actions ainsi que leurs consÃ©quences, vÃ©rifie quelles actions sont autorisÃ©es Ã  un moment donnÃ©... GÃ¨re les validations des mouvements (dÃ©placement autorisÃ© ou non), les captures de pions, et la dÃ©tection des conditions de victoire.
- `renderer.py`: Responsable de l'affichage du jeu.
- `reward.py`: ImplÃ©mente les fonctions de rÃ©compense pour guider lâ€™apprentissage des agents.
Les rÃ©compenses peuvent Ãªtre basÃ©es sur :
La progression des pions sur le plateau.
La capture dâ€™un pion adverse.
Lâ€™atteinte de la zone dâ€™arrivÃ©e.
- `state.py`: DÃ©finit les Ã©tats dans lesquels peuvent se trouver les pions.

### `db/`

Le dossier `db/` regroupe tous les Ã©lÃ©ments nÃ©cessaires pour gÃ©rer la base de donnÃ©es associÃ©e au projet, les scripts pour collecter, transformer et analyser les donnÃ©es, ainsi que les donnÃ©es utilisÃ©es pour nos analyses.

```bash
db/
â”œâ”€â”€ analyse/
â”‚   â”œâ”€â”€ analyse_agents.ipynb
â”‚   â””â”€â”€ analyse_entraÃ®nement.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ action_stats.csv
â”‚   â”œâ”€â”€ game.csv
â”‚   â”œâ”€â”€ game_rule.csv
â”‚   â”œâ”€â”€ is_rule_of.csv
â”‚   â”œâ”€â”€ participant.csv
â”‚   â”œâ”€â”€ player.csv
â”‚   â””â”€â”€ set_of_rules.csv
â”œâ”€â”€ secret/                     # Dossier Ã  crÃ©er en local
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ config_game.py
â”œâ”€â”€ export.py
â”œâ”€â”€ insert.py
â”œâ”€â”€ ludo_stats_play.py
â”œâ”€â”€ notes_db.md
â”œâ”€â”€ rules.py
â”œâ”€â”€ schema.py
â”œâ”€â”€ Ubuntu_db_configuration_and_setup.md
â””â”€â”€ Windows_db_configuration_and_setup.md
```
- `analyse/` : Dossier contenant les notebooks d'analyse des agents
    - `analyse_agents.ipynb` : Notebook d'analyse de performance des agents entraiÌ‚neÌs en fonction des configurations de jeu.
    - `analyse_entraÃ®nement.ipynb` : Notebook d'analyse de l'entrainement des agents
- `data/` : Dossier contenant les fichiers csv de donnÃ©es exportÃ©es depuis la base de donnÃ©es  et utilisÃ©s pour les analyses.
- `secret/config.py` : Fichier de configuration contenant l'URL de connexion Ã  la base de donnÃ©es PostgreSQL. Ce dossier est Ã  crÃ©er localement.
- `config_game.py` : Fichier contenant les fonctions nÃ©cessaires pour gÃ©nÃ©rer les configurations de jeu des parties entre agents.
- `export.py`: Script permettant d'exporter les donnÃ©es de la base de donnÃ©es au format CSV.
- `insert.py` : Fichier contenant les classes permettant d'insÃ©rer les donnÃ©es dans la base de donnÃ©es.
- `ludo_stats_play.py` : Scripts utilisÃ©s pour simuler les parties entre agents et enregistrer les donnÃ©es liÃ©es dans la base de donnÃ©es.  
Ce fichier contient plusieurs fonctions mains que nous avons utilisÃ©es selon nos besoins :
    - `main()` : Permet de lancer une ou plusieurs parties en dÃ©finissant depuis le terminal : le nombre de joueurs, le nombre de pions, la configuration de rÃ¨gles, les agents a utiliser et le nombre de parties Ã  lancer.
    - `main_auto()` : Permet de lancer automatiquement plusieurs parties entre des agents identiques (mÃªme type et mÃªme nombre de pas d'entrainement).  
    Il faut prÃ©ciser la configuration de rÃ¨gles, le nombre de joueurs, le nombre de chevaux et le nombre de parties Ã  lancer.  
    Lance toutes les parties pour tous les agents dÃ©finis correspondant au nombre de joueurs, de pions et Ã  la configuration spÃ©cifiÃ©e. 
    - `main_lancer_parties_pour_analyse_entrainement()` : Permet d'exÃ©cuter les parties gÃ©nÃ©rant les donnÃ©es nÃ©cessaires Ã  l'analyse de l'entrainement des agents.
    - `main_lancer_auto_mathcups()`: Permet d'exÃ©cuter les parties gÃ©nÃ©rant les donnÃ©es nÃ©cessaires Ã  l'analyse de des agents entraÃ®nÃ©s en fonction des configurations de jeu.
- `Ubuntu_db_configuration_and_setup.md` et `Windows_db_configuration_and_setup.md` : Fichiers fournissant les informations pour configurer et utiliser la base de donnÃ©es *ludo_stats*.
- `rules.py` : Fichier permettant de gÃ©rer les rÃ¨gles (dÃ©finition, description et dÃ©termination dynamique).
- `schema.py` : Script permettant d'initialiser la base de donnÃ©es en crÃ©ant les tables nÃ©cessaires.


---

Toutes les installations et fonctionnalitÃ©s ont Ã©tÃ© testÃ©es sur Windows et Ubuntu. Sur macOS, tout a Ã©tÃ© vÃ©rifiÃ©, Ã  l'exception de l'installation de PostgreSQL, qui n'est pas requise pour exÃ©cuter le jeu et effectuer les analyses.

## ğŸ“‘ Documentation complÃ¨te
Pour une vue dâ€™ensemble complÃ¨te de la mÃ©thodologie, des objectifs pÃ©dagogiques et techniques, des choix dâ€™implÃ©mentation, de lâ€™analyse des rÃ©sultats et des discussions dÃ©taillÃ©es, veuillez consulter le [rapport complet du projet (en franÃ§ais)](./resources/project-report-fr.pdf) disponible dans ce dÃ©pÃ´t.

## ğŸ‘· Contributeurs
- MARQUIS ZoÃ©
- KRUZIC Charlotte
- KUDRIASHOV Daniil
- ZAITCEVA Ekaterina
